Model,LC_Win_Rate,Win_Rate
Yi-Large-preview,51.9,57.5
Claude 3 Opus,40.5,29.1
Meta-Llama-3.1-405b-Instruct,39.3,39.1
GPT-4,38.1,23.6
Qwen2-72B-Instruct,38.1,29.9
Meta-Llama-3.1-70b-Instruct,38.1,39.1
Qwen1.5-72B-Chat,36.6,26.5
Claude 3.5 Sonnet,34.9,25.6
Llama-3-70b-Instruct,34.4,33.2
Mistral-Large-2402,32.7,21.4
Mixtral-8x22b-Instruct-v0.1,30.9,22.2
Mistral Medium,28.6,21.9
Claude-2.0,28.2,17.2
Yi-34B-Chat,27.2,29.7
DBRX-Instruct-Preview,25.4,18.4
Claude-2.1,25.3,15.7
Gemini Pro,24.4,18.2
Qwen1.5-14B-Chat,23.9,18.6
Mixtral-8x7b-Instruct-v0.1,23.7,18.3
Llama-3-8b-Instruct,22.9,22.6
Meta-Llama-3.1-8b-Instruct,20.9,21.8
OpenHermes-2.5-Mistral-7b,16.2,10.3
Qwen1.5-7B-Chat,14.7,11.8
Vicuna-13B,9.2,5.8
Vicuna-7B,6.3,4.2
Guanaco-33B,5.7,5.0
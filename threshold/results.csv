LC AlpacaEval 2.0, 0.845992, 0.117670, 0.153285, 0.186097, 0.155700, 0.469659, 0.692198
AlpacaEval 2.0, 0.793300, 0.165025, 0.052271, 0.085249, 0.271909, 0.573100, 0.733349
AlpacaEval 1.0, 0.556446, -0.240192, -0.290957, -0.051031, -0.106112, 0.208277, 0.309368
MT-bench, 0.764647, 0.183444, 0.315477, 0.550283, 0.719799, 0.764647, 0.764647
WildBench, 0.829747, NaN, NaN, NaN, 0.242536, 0.242536, 0.447214
Open LLM, 0.458732, -0.203653, -0.124035, -0.176777, -0.110030, 0.050880, 0.280661
ARC-C, 0.679956, -0.188982, -0.125000, 0.145479, 0.316721, 0.396908, 0.498353
HellaSwag, 0.470763, 0.133235, 0.148939, 0.072075, 0.162747, 0.313356, 0.464446
MMLU, 0.684666, -0.121268, -0.027298, -0.018487, 0.053252, 0.097811, 0.399026
TruthfulQA, 0.340574, 0.111111, -0.174078, 0.000000, 0.093153, 0.098725, 0.236068
WinoGrande, 0.535653, -0.101929, -0.168550, 0.064550, 0.161690, 0.281775, 0.483017
GSM-8K, 0.441155, -0.107211, -0.107211, -0.105409, -0.159152, 0.062807, 0.136676
AGI Eval, 0.650673, 0.357771, 0.288675, 0.289683, 0.032898, 0.248817, 0.487446
BBH, cot, 0.702474, 0.166667, 0.111803, 0.157622, 0.309016, 0.151307, 0.341565
HumanEval, 0.670166, 0.000000, 0.166667, 0.000000, 0.000000, 0.202113, 0.242272
LLMonitor, 0.514710, 0.000000, 0.000000, -0.084515, 0.000000, 0.102869, 0.254462
Output Length, 0.203593, 0.000000, 0.000000, -0.114708, 0.000000, 0.062994, 0.125000
Arena Hard, 0.725512, -0.069505, -0.053376, 0.000000, 0.165145, 0.273760, 0.409673
MixEval-Hard, 0.818025, -0.235702, -0.286039, -0.141421, 0.050637, 0.457869, 0.662059
MixEval, 0.762566, 0.000000, 0.095346, 0.077152, 0.253185, 0.274721, 0.569301
